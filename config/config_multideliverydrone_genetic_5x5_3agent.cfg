# Config file for grid world environment

# This file includes parameters that are needed to run the simulation.
############################################################################
# Problem Solving Configurations
############################################################################

ENVIRONMENT				= multideliverydrone

# Agent Type
# Options: q-iteration, trajectory-based-value-iteration

AGENT					= trajectory-based-value-iteration-multi-agent

# Representation type
# Options: TabularStateActionPair, RepresentationUDP

REPRESENTATION 			= RepresentationUDP

# Deep Learning Types
# DeepQNetwork_PrioritizedReplay_Target, DeepQNetwork_PrioritizedReplay, DeepCorrection

DEEP_ALGO_TYPE			= GeneticDistributer

# Algo modes for GeneticDistributer: static, random, search, genetic
ALGO_MODE               = search

AGENT_MODEL				= models/model_0_20201118_105805.h5

############################################################################
# Environment Configurations
############################################################################

# Number of agents
# This parameter indicates how many agents will be spawned in this problem.

NUMBER_OF_AGENTS		=	3

# Initial State
# This parameter indicates where the agents will be spawned.
# INITIAL_STATE			=	0,0;4,0;4,4;3,2

#INITIAL_STATE			=	0,0,6,1,1,4,3


# Number of rows
# This parameter indicates how many rows will be considered in this problem.

NUMBER_OF_ROWS			=	5

# Number of columns
# This parameter indicates how many columns will be considered in this problem.

NUMBER_OF_COLUMNS		=	5

# Locations of Terminal points, such as goal or holes.
# Special states that have different rewards.
# Ex: goal1;goal2;.... where goali = state[0],state[1],...
# Ex: 1,4;1,1;4,0;3,2

TERMINAL_STATES			= 3,4

# Rewards
# Rewards for each terminal states given by LOCATION_OF_TERMINALS
# respectively.
# Ex: 1,-1,-1,-1

REWARDS					= 	1

# Cost of action
# This parameter shows the value of the cost for each action that is taken.

COST_ACTION				= 	-0.04

# Center of Blocked States
# Special states that can not be reached.
# Ex: block1;block2;.... where blocki = state[0],state[1],...
# BLOCKED_STATES			= 2,0;0,2;2,3;2,4;4,2;4,3

BLOCKED_STATES			= 2,2

# Center of Refuel Stations
# Special states that can not be reached.
# Ex: station1;station2;.... where stationi = state[0],state[1],...
# REFUEL_STATES			= 2,0;0,2;2,3;2,4;4,2;4,3

REFUEL_STATES			= 1,3;2,1;4,2

# Maximum fuel when visited a refueling station
# This parameter shows the value of the fuel that will be loaded when visited a station.

FUEL_MAX				= 	6

# Probability Of Applying a Random Action Default:0.2
# Ex:
# Proper Action: 1-PROBABILITY_OF_RANDOM_ACTION
# Turn Left: PROBABILITY_OF_RANDOM_ACTION/2
# Turn Right: PROBABILITY_OF_RANDOM_ACTION/2

PROBABILITY_OF_RANDOM_ACTION = 0.0

############################################################################
# Fundamental Agent Configurations
############################################################################

# Epsilon
# This parameter terminates the current iteration if update is smaller
# than it.

EPSILON					= 	1e-6f

# Gamma - Discount Factor
# This parameter balances current and future rewards.

GAMMA					= 	0.8

# Max Iteration (Episodes)
# Total number of iteration to find the optimal policy.

MAX_NUMBER_OF_ITERATION	= 	10000

# Number of simulations per iteration
# It helps us to evaluate the performance of the agent by doing
# simulations in series. Default:20

NUMBER_OF_SIMULATIONS	= 	1

# It prevents the agent to stuck in a loop in the environment
# It is recommended that to keep it high to know whether it
# is stucked in the cumulative rewards plot.

MAX_STEPS_IN_SIMULATION = 	25

# Defines how many bellman updates should be discarded to make  a series of
# simulations to show the performance of agent. Default: 1000

BELLMAN_STRIDE_FORSIMULATION =  2000

############################################################################
# Trajectory Based Value Iteration Related Configurations
############################################################################

# Explore-Exploit Parameter.
# Probability that balances exploration and exploitation

EPSILON_PROBABILITY		= 0.2

# Epsilon Probability decay rate for every epoch.
# Default: 0.998 1.0 will disable it also

EPSILON_PROBABILITY_DECAYRATE = 1.0

# Number of next state samples Default: 100
# It will be used to approximate the expectations on the next state.

SAMPLE_LENGTH_L1		= 1

# Trajectory Length Default:NUMBER_OF_ROWS*NUMBER_OF_COLUMNS
# It will be used to determine the trajectory that the algorithm will follow for each iteration.

LENGTH_OF_TRAJECTORY	= 25

############################################################################
# Basis Function Approximator Related Configurations
############################################################################

# Center of Features
# Feature values that are located at those state values.
# Ex: feature1;feature2;.... where featurei = state[0],state[1],...

FEATURES				= 0,0;0,2;1,0;1,2;2,2;2,3

# Theta Weight Learning rate
# Alpha is the learning parameter of gradient descent that is utilized
# in basis functions of function approximator classes.

ALPHA 					= 0.4

############################################################################
# Neural Network Function Approximator Related Configurations
############################################################################

# Hidden Layer Parameters
# This parameters configures the total number of hidden layers and how many
# neurons in each of them.
# Ex: for three hidden layer with 6, 4 and 3 neurons respectively: 6;4;3
# Ex: for single hidden layer with 5 neurons just enter: 5

HIDDEN_LAYERS			= 32;64;32

# Learning rate for gradient descent algorithm used in backprogation phase.
# Default value: 0.15

ETA_LEARNING_RATE		= 0.01

# Batch size for learning multiple samples at a time for neural networks.

BATCH_SIZE				= 32

# It determines how many backpropagation training will be done for
# current batch. Namely, Epoch

TRAINING_PASS_PER_BATCH	= 1

EXPERIENCE_REPLAY_BUFFER = 10240

############################################################################
# UDP Represantation Related Configurations
############################################################################

# The IP of machine that runs external representation

TARGET_HOST			= 127.0.0.1

# The transmit port of agent that will communicate with external
# representation

TRANSMIT_PORT		= 5001

# The receiver port of agent that will communicate with external
# representation

RECEIVE_PORT		= 4001
