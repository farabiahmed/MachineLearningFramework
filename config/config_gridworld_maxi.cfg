# Config file for grid world environment

# This file includes parameters that are needed to run the simulation. 

############################################################################
# Environment Configurations
############################################################################

# Number of rows
# This parameter indicates how many rows will be considered in this problem.

NUMBER_OF_ROWS			=	12

# Number of columns
# This parameter indicates how many columns will be considered in this problem.

NUMBER_OF_COLUMNS		=	12

# Locations of Terminal points, such as goal or holes. 
# Special states that have different rewards.
# Ex: goal1;goal2;.... where goali = state[0],state[1],...

TERMINAL_STATES			= 0,3;1,3;3,1;5,6;9,11;8,2;3,7

# Rewards
# Rewards for each terminal states given by LOCATION_OF_TERMINALS 
# respectively.

REWARDS					= 	1,-1,-1,-1,1,1,-1

# Cost of action
# This parameter shows the value of the cost for each action that is taken.

COST_ACTION				= 	-0.04

# Center of Blocked States
# Special states that can not be reached.
# Ex: block1;block2;.... where blocki = state[0],state[1],...

BLOCKED_STATES			= 1,1;8,1;7,5;3,10

# Probability Of Applying a Random Action
# Ex:
# Proper Action: 1-PROBABILITY_OF_RANDOM_ACTION
# Turn Left: PROBABILITY_OF_RANDOM_ACTION/2
# Turn Right: PROBABILITY_OF_RANDOM_ACTION/2

PROBABILITY_OF_RANDOM_ACTION = 0.20

############################################################################
# QIteration Agent Configurations
############################################################################

# Epsilon
# This parameter terminates the current iteration if update is smaller
# than it.

EPSILON					= 	1e-5f

# Gamma - Discount Factor
# This parameter balances current and future rewards.

GAMMA					= 	0.8

# Max Iteration
# Total number of iteration to find the optimal policy.

MAX_NUMBER_OF_ITERATION	= 	5000

############################################################################
# Trajectory Based Value Iteration Related Configurations
############################################################################

# Explore-Exploit Parameter.
# Probability that balances exploration and exploitation

EPSILON_PROBABILITY		= 0.15

# Number of next state samples
# It will be used to approximate the expectations on the next state.

SAMPLE_LENGTH_L1		= 100

# Trajectory Length
# It will be used to determine the trajectory that the algorithm will follow for each iteration.

LENGTH_OF_TRAJECTORY	= 100

############################################################################
# Basis Function Approximator Related Configurations
############################################################################

# Center of Features
# Feature values that are located at those state values. 
# Ex: feature1;feature2;.... where featurei = state[0],state[1],...

FEATURES				= 0,0;0,2;1,0;1,2;2,2;2,3
#FEATURES				= 0,0;0,1;0,2;0,3;1,0;1,1;1,2;1,3;2,0;2,1;2,2;2,3

# Theta Weight Learning rate
# Alpha is the learning parameter of gradient descent that is utilized
# in basis functions of function approximator classes.

ALPHA 					= 0.4

############################################################################
# Neural Network Function Approximator Related Configurations
############################################################################

# Hidden Layer Parameters
# This parameters configures the total number of hidden layers and how many
# neurons in each of them.
# Ex: for three hidden layer with 6, 4 and 3 neurons respectively: 6;4;3
# Ex: for single hidden layer with 5 neurons just enter: 5
 
HIDDEN_LAYERS			= 4

# Learning rate for gradient descent algorithm used in backprogation phase.
# Default value: 0.15

ETA_LEARNING_RATE		= 0.15