# Config file for grid world environment

# This file includes parameters that are needed to run the simulation. 

############################################################################
# Environment Configurations
############################################################################

# Number of agents
# This parameter indicates how many agents will be spawned in this problem.

NUMBER_OF_AGENTS		=	3

# Initial State
# This parameter indicates where the agents will be spawned.
# INITIAL_STATE			=	0,0;4,0;4,4;3,2

INITIAL_STATE			=	0,0;4,0;4,4


# Number of rows
# This parameter indicates how many rows will be considered in this problem.

NUMBER_OF_ROWS			=	5

# Number of columns
# This parameter indicates how many columns will be considered in this problem.

NUMBER_OF_COLUMNS		=	5

# Locations of Terminal points, such as goal or holes. 
# Special states that have different rewards.
# Ex: goal1;goal2;.... where goali = state[0],state[1],...
# Ex: 1,4;1,1;4,0;3,2

TERMINAL_STATES			= 1,4

# Rewards
# Rewards for each terminal states given by LOCATION_OF_TERMINALS 
# respectively.
# Ex: 1,-1,-1,-1

REWARDS					= 	1

# Cost of action
# This parameter shows the value of the cost for each action that is taken.
# COST_ACTION				= 	-0.04

COST_ACTION				= 	-0.04

# Center of Blocked States
# Special states that can not be reached.
# Ex: block1;block2;.... where blocki = state[0],state[1],...
# BLOCKED_STATES			= 2,0;0,2;2,3;2,4;4,2;4,3

BLOCKED_STATES			= 2,0

# Probability Of Applying a Random Action Default:0.2
# Ex:
# Proper Action: 1-PROBABILITY_OF_RANDOM_ACTION
# Turn Left: PROBABILITY_OF_RANDOM_ACTION/2
# Turn Right: PROBABILITY_OF_RANDOM_ACTION/2

PROBABILITY_OF_RANDOM_ACTION = 0.0

############################################################################
# Fundamental Agent Configurations
############################################################################

# Epsilon
# This parameter terminates the current iteration if update is smaller
# than it.

EPSILON					= 	1e-6f

# Gamma - Discount Factor
# This parameter balances current and future rewards.

GAMMA					= 	0.99

# Max Iteration (Episodes)
# Total number of iteration to find the optimal policy.

MAX_NUMBER_OF_ITERATION	= 	20000

# Number of simulations per iteration
# It helps us to evaluate the performance of the agent by doing 
# simulations in series. Default:20

NUMBER_OF_SIMULATIONS	= 	5

# It prevents the agent to stuck in a loop in the environment
# It is recommended that to keep it high to know whether it
# is stucked in the cumulative rewards plot. 

MAX_STEPS_IN_SIMULATION = 	100

# Defines how many bellman updates should be discarded to make  a series of 
# simulations to show the performance of agent. Default: 1000

BELLMAN_STRIDE_FORSIMULATION =  2000

############################################################################
# Trajectory Based Value Iteration Related Configurations
############################################################################

# Explore-Exploit Parameter.
# Probability that balances exploration and exploitation

EPSILON_PROBABILITY		= 0.2

# Epsilon Probability decay rate for every epoch. 
# Default: 0.998

EPSILON_PROBABILITY_DECAYRATE = 0.9998

# Number of next state samples Default: 100
# It will be used to approximate the expectations on the next state.

SAMPLE_LENGTH_L1		= 1

# Trajectory Length Default:NUMBER_OF_ROWS*NUMBER_OF_COLUMNS
# It will be used to determine the trajectory that the algorithm will follow for each iteration.

LENGTH_OF_TRAJECTORY	= 64

############################################################################
# Basis Function Approximator Related Configurations
############################################################################

# Center of Features
# Feature values that are located at those state values. 
# Ex: feature1;feature2;.... where featurei = state[0],state[1],...

FEATURES				= 0,0;0,2;1,0;1,2;2,2;2,3

# Theta Weight Learning rate
# Alpha is the learning parameter of gradient descent that is utilized
# in basis functions of function approximator classes.

ALPHA 					= 0.4

############################################################################
# Neural Network Function Approximator Related Configurations
############################################################################

# Hidden Layer Parameters
# This parameters configures the total number of hidden layers and how many
# neurons in each of them.
# Ex: for three hidden layer with 6, 4 and 3 neurons respectively: 6;4;3
# Ex: for single hidden layer with 5 neurons just enter: 5
 
HIDDEN_LAYERS			= 12;6

# Learning rate for gradient descent algorithm used in backprogation phase.
# Default value: 0.15

ETA_LEARNING_RATE		= 0.1

# Batch size for learning multiple samples at a time for neural networks.

BATCH_SIZE				= 1

# It determines how many backpropagation training will be done for 
# current batch. Namely, Epoch

TRAINING_PASS_PER_BATCH	= 1

############################################################################
# UDP Represantation Related Configurations
############################################################################

# The IP of machine that runs external representation 

TARGET_HOST			= 127.0.0.1

# The transmit port of agent that will communicate with external 
# representation

TRANSMIT_PORT		= 5000

# The receiver port of agent that will communicate with external 
# representation

RECEIVE_PORT		= 4000