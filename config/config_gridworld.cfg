# Config file for grid world environment

# This file includes parameters that are needed to run the simulation. 

############################################################################
# Environment Configurations
############################################################################

# Number of states
# This parameter forms the Qvalue (<state-action> pair) container.

NUMBER_OF_STATES		=	25

# Number of action
# This parameter forms the Qvalue (<state-action> pair) container.

NUMBER_OF_ACTIONS		=	4

# Number of rows
# This parameter indicates how many rows will be considered in this problem.

NUMBER_OF_ROWS			=	5

# Number of columns
# This parameter indicates how many columns will be considered in this problem.

NUMBER_OF_COLUMNS		=	5

# Locations of Terminal points, such as goal or holes. 
# Special states that have different rewards.
# Ex: goal1;goal2;.... where goali = state[0],state[1],...

TERMINAL_STATES			= 0,3;1,3;3,1

# Rewards
# Rewards for each terminal states given by LOCATION_OF_TERMINALS 
# respectively.

REWARDS					= 	1,-1,-1

# Cost of action
# This parameter shows the value of the cost for each action that is taken.

COST_ACTION				= 	-0.04

# Center of Blocked States
# Special states that can not be reached.
# Ex: block1;block2;.... where blocki = state[0],state[1],...

BLOCKED_STATES			= 1,1

# Probability Of Applying a Random Action
# Ex:
# Proper Action: 1-PROBABILITY_OF_RANDOM_ACTION
# Turn Left: PROBABILITY_OF_RANDOM_ACTION/2
# Turn Right: PROBABILITY_OF_RANDOM_ACTION/2

PROBABILITY_OF_RANDOM_ACTION = 0.20

############################################################################
# QIteration Agent Configurations
############################################################################

# Epsilon
# This parameter terminates the current iteration if update is smaller
# than it.

EPSILON					= 	1e-5f

# Gamma - Discount Factor
# This parameter balances current and future rewards.

GAMMA					= 	0.8

# Max Iteration
# Total number of iteration to find the optimal policy.

MAX_NUMBER_OF_ITERATION	= 	50

############################################################################
# Trajectory Based Value Iteration Related Configurations
############################################################################

# Explore-Exploit Parameter.
# Probability that balances exploration and exploitation

EPSILON_PROBABILITY		= 0.8

# Number of next state samples
# It will be used to approximate the expectations on the next state.

SAMPLE_LENGTH_L1		= 100

# Trajectory Length
# It will be used to determine the trajectory that the algorithm will follow for each iteration.

LENGTH_OF_TRAJECTORY	= 100

# Number of Features
# Total number of features are needed to form phi and theata vectors.

NUMBER_OF_FEATURES		= 6

# Center of Features
# Feature values that are located at those state values. 
# Ex: feature1;feature2;.... where featurei = state[0],state[1],...

FEATURE_LOCATIONS		= 0,0;0,2;1,0;1,2;2,2;2,3