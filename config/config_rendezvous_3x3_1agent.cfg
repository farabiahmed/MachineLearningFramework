# Config file for grid world environment

# This file includes parameters that are needed to run the simulation. 
############################################################################
# Problem Solving Configurations
############################################################################

ENVIRONMENT				= rendezvous

# Agent Type
# Options: q-iteration, trajectory-based-value-iteration

AGENT					= trajectory-based-value-iteration

# Representation type
# Options: TabularStateActionPair, RepresentationUDP

REPRESENTATION 			= RepresentationUDP

# Deep Learning Types
# DeepQNetwork_PrioritizedReplay_Target, DeepQNetwork_PrioritizedReplay, DeepCorrection

DEEP_ALGO_TYPE			= DeepQNetwork_PrioritizedReplay_Target_LearnerThread_Hybrid

############################################################################
# Environment Configurations
############################################################################

# Number of agents
# This parameter indicates how many agents will be spawned in this problem.

NUMBER_OF_AGENTS		=	1

# Initial State
# This parameter indicates where the agents will be spawned.

INITIAL_STATE			=	0,0

# Number of rows
# This parameter indicates how many rows will be considered in this problem.

NUMBER_OF_ROWS			=	3

# Number of columns
# This parameter indicates how many columns will be considered in this problem.

NUMBER_OF_COLUMNS		=	3

# Locations of Terminal points, such as goal or holes. 
# Special states that have different rewards.
# Ex: goal1;goal2;.... where goali = state[0],state[1],...

TERMINAL_STATES			= 1,2

# Rewards
# Rewards for each terminal states given by LOCATION_OF_TERMINALS 
# respectively.

REWARDS					= 	1

# Cost of action
# This parameter shows the value of the cost for each action that is taken.

COST_ACTION				= 	-0.04

# Center of Blocked States
# Special states that can not be reached.
# Ex: block1;block2;.... where blocki = state[0],state[1],...

BLOCKED_STATES			= 1,0

# Probability Of Applying a Random Action Default:0.2
# Ex:
# Proper Action: 1-PROBABILITY_OF_RANDOM_ACTION
# Turn Left: PROBABILITY_OF_RANDOM_ACTION/2
# Turn Right: PROBABILITY_OF_RANDOM_ACTION/2

PROBABILITY_OF_RANDOM_ACTION = 0.0

############################################################################
# Fundamental Agent Configurations
############################################################################

# Epsilon
# This parameter terminates the current iteration if update is smaller
# than it.

EPSILON					= 	1e-6f

# Gamma - Discount Factor
# This parameter balances current and future rewards.

GAMMA					= 	0.99

# Max Iteration (Episodes)
# Total number of iteration to find the optimal policy.

MAX_NUMBER_OF_ITERATION	= 	2000

# Number of simulations per iteration
# It helps us to evaluate the performance of the agent by doing 
# simulations in series. Default:20

NUMBER_OF_SIMULATIONS	= 	1

# It prevents the agent to stuck in a loop in the environment
# It is recommended that to keep it high to know whether it
# is stucked in the cumulative rewards plot. 

MAX_STEPS_IN_SIMULATION = 	100

# Defines how many bellman updates should be discarded to make  a series of 
# simulations to show the performance of agent. Default: 1000

BELLMAN_STRIDE_FORSIMULATION = 	250

############################################################################
# Trajectory Based Value Iteration Related Configurations
############################################################################

# Explore-Exploit Parameter.
# Probability that balances exploration and exploitation

EPSILON_PROBABILITY		= 0.2

# Epsilon Probability decay rate for every epoch. 
# Default: 0.998

EPSILON_PROBABILITY_DECAYRATE = 1

# Number of next state samples Default: 100
# It will be used to approximate the expectations on the next state.

SAMPLE_LENGTH_L1		= 1

# Trajectory Length Default:NUMBER_OF_ROWS*NUMBER_OF_COLUMNS
# It will be used to determine the trajectory that the algorithm will follow for each iteration.

LENGTH_OF_TRAJECTORY	= 25

############################################################################
# Basis Function Approximator Related Configurations
############################################################################

# Center of Features
# Feature values that are located at those state values. 
# Ex: feature1;feature2;.... where featurei = state[0],state[1],...

FEATURES				= 0,0;0,2;1,0;1,2;2,2;2,3

# Theta Weight Learning rate
# Alpha is the learning parameter of gradient descent that is utilized
# in basis functions of function approximator classes.

ALPHA 					= 0.4

############################################################################
# Neural Network Function Approximator Related Configurations
############################################################################

# Hidden Layer Parameters
# This parameters configures the total number of hidden layers and how many
# neurons in each of them.
# Ex: for three hidden layer with 6, 4 and 3 neurons respectively: 6;4;3
# Ex: for single hidden layer with 5 neurons just enter: 5
 
HIDDEN_LAYERS			= 12;12

# Learning rate for gradient descent algorithm used in backprogation phase.
# Default value: 0.15

ETA_LEARNING_RATE		= 0.1

# Batch size for learning multiple samples at a time for neural networks.

BATCH_SIZE				= 16

# It determines how many backpropagation training will be done for 
# current batch.

TRAINING_PASS_PER_BATCH	= 1

EXPERIENCE_REPLAY_BUFFER = 128

############################################################################
# UDP Represantation Related Configurations
############################################################################

# The IP of machine that runs external representation 

TARGET_HOST			= 127.0.0.1

# The transmit port of agent that will communicate with external 
# representation

TRANSMIT_PORT		= 5000

# The receiver port of agent that will communicate with external 
# representation

RECEIVE_PORT		= 4000